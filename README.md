# Coragem Multi-Tenant RAG API ğŸš€

A production-grade, multi-tenant Retrieval-Augmented Generation (RAG) API built with FastAPI. It empowers employees across different departments to query internal documents securely using advanced AI, with strict data isolation, conversational memory, and real-time streaming capabilities.

---

## ğŸŒŸ Key Features

### ğŸ”’ Enterprise Security & Isolation
- **Department-Level Data Isolation**: Users can only retrieve documents and context belonging to their specific department. A Sales rep cannot query HR policies.
- **Role-Based Access Control (RBAC)**: JWT-based authentication ensuring secure access to API endpoints.
- **Rate Limiting**: Integrated `slowapi` to restrict requests per user and department, preventing abuse and managing API costs.

### ğŸ§  Advanced RAG Architecture
- **Provider-Agnostic Embeddings**: Seamlessly switch between Google Gemini (`gemini-embedding-001`) and Azure OpenAI (`text-embedding-3-small` / `text-embedding-ada-002`) using a dynamic Factory Pattern.
- **Conversational Memory**: Redis-backed session history (`session_id`) allows the LLM to remember multi-turn conversations naturally.
- **Server-Sent Events (SSE) Streaming**: The `/query/stream` endpoint yields tokens dynamically as they are generated by the LLM for a snappy, ChatGPT-like frontend experience.
- **Hallucination Prevention**: Strict confidence thresholds prevent the model from guessing answers when semantic similarity is too low.

### âš¡ Performance & Optimization
- **Batch Processing**: Document chunking and embedding generation pipeline is purely asynchronous and batched, boasting 10x faster ingestion into the Pinecone Vector Database.
- **Redis Caching**: Frequently asked questions are cached at the department level to return instant results (cache hit) and bypass expensive Pinecone/LLM calls.
- **Prometheus Observability**: Application exposes `/metrics` for tracking `rag_queries_total`, `query_latency_seconds`, and usage analytics.

---

## ğŸ› ï¸ Technology Stack

- **Framework:** FastAPI (Python 3.12)
- **Vector Database:** Pinecone (Serverless)
- **LLM/Embeddings:** Google Gemini / Azure OpenAI
- **Relational DB:** PostgreSQL (Async SQLAlchemy)
- **Caching & Memory:** Redis
- **Infra/Deployment:** Docker Compose, Uvicorn

---

## ğŸš¦ Getting Started (Local Development)

### 1. Prerequisites
Ensure you have the following installed:
- Docker and Docker Compose
- `uv` (Python Package Manager if running natively)

### 2. Environment Configuration
Create a `.env` file in the root directory based on `.env.example`:

```env
# Application
ENVIRONMENT=development
CORS_ORIGINS=["http://localhost:3000"]
ALLOWED_HOSTS=["*"]

# Security
SECRET_KEY=your_super_secret_jwt_key
ACCESS_TOKEN_EXPIRE_MINUTES=60
COMPANY_NAME=Coragem

# Databases
DATABASE_URL=postgresql+asyncpg://postgres:postgres@db:5432/rag_db
REDIS_URL=redis://redis:6379/0

# Pinecone
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=documents

# LLM Providers (Gemini is enabled by default)
EMBEDDING_PROVIDER=gemini  # Or `azure_openai`
GEMINI_API_KEY=your_gemini_api_key

# Azure OpenAI (If enabled)
AZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com/
AZURE_OPENAI_API_KEY=your_azure_api_key
AZURE_OPENAI_DEPLOYMENT_NAME=text-embedding-3-small
```

### 3. Run with Docker Compose
The easiest way to boot the entire stack (API, PostgreSQL, Redis) is via Docker Compose:

```bash
docker compose up --build -d
```

### 4. Database Migrations
Run Alembic migrations to set up your PostgreSQL tables:

```bash
docker exec -it api alembic upgrade head
```

The API will now be available at: **http://localhost:8000**
Interactive Swagger Documentation: **http://localhost:8000/docs**

---

## ğŸ”Œ Core API Endpoints

### Authentication
- `POST /api/v1/auth/login`: Authenticate and receive a JWT access token.
- `POST /api/v1/auth/register`: Register a new employee with a specific department assigning.

### Document Management
- `POST /api/v1/documents/ingest/text`: Ingest raw text directly into Pinecone and Postgres.
- `POST /api/v1/documents/ingest/pdf`: Upload and parse a PDF, splitting it into semantic chunks for vector storage.

### RAG Chat & Querying
Requires `Authorization: Bearer <token>`

**Standard JSON Query:**
```bash
curl -X POST "http://localhost:8000/api/v1/rag/query" \
-H "Authorization: Bearer YOUR_TOKEN" \
-H "Content-Type: application/json" \
-d '{
  "query": "What are the rules for maternal leave?",
  "session_id": "hr-session-01",
  "confidence_threshold": 0.5
}'
```

**Real-Time Streaming Query (SSE):**
```bash
curl -N -X POST "http://localhost:8000/api/v1/rag/query/stream" \
-H "Authorization: Bearer YOUR_TOKEN" \
-H "Content-Type: application/json" \
-d '{
  "query": "Summarize the Q3 product catalog.",
  "session_id": "sales-session-99",
  "confidence_threshold": 0.5
}'
```

---

## ğŸ—ï¸ Project Structure
```text
api/
â”œâ”€â”€ apps/               # Business Logic Domains
â”‚   â”œâ”€â”€ auth/           # JWT, Users, RBAC
â”‚   â”œâ”€â”€ documents/      # Ingestion, Parsing, Chunking
â”‚   â”œâ”€â”€ rag/            # Vector Search, LLM Generation, SSE Stream
â”‚   â””â”€â”€ health/         # System statuses
â”œâ”€â”€ core/               # App Infrastructure
â”‚   â”œâ”€â”€ cache.py        # Redis Memory & Query caching
â”‚   â”œâ”€â”€ embeddings.py   # Factory pattern for multi-vendor Embeddings
â”‚   â”œâ”€â”€ llm.py          # LLM connection and Prompt Engineering
â”‚   â”œâ”€â”€ vector_store.py # Pinecone search/upsert operations
â”‚   â””â”€â”€ config.py       # Pydantic Settings
â”œâ”€â”€ db/                 # PostgreSQL Async Session Management
â””â”€â”€ utils/              # Utilities (Logging, PDF parsing, Errors)
```
